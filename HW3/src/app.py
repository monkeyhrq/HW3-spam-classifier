import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import joblib
import os 
from pathlib import Path 
from utils import load_and_preprocess_data, train_and_save_model, evaluate_model, predict_message, get_top_tokens
from sklearn.metrics import precision_score, recall_score, f1_score # ç‚ºäº†é–¾å€¼æƒææ–°å¢

# --- è¨­å®šè·¯å¾‘å’Œå¸¸æ•¸ ---

# ç²å– app.py è…³æœ¬çš„çµ•å°è·¯å¾‘
current_file_path = Path(__file__).absolute() 

# å‘ä¸Šå…©å±¤åˆ°é” HW3 æ ¹ç›®éŒ„ (å‡è¨­ app.py åœ¨ src/ å…§)
BASE_DIR = current_file_path.parent.parent 

# ä½¿ç”¨ Path æ§‹é€ æ•¸æ“šé›†è·¯å¾‘
DATA_PATH = BASE_DIR / 'Chapter03' / 'datasets' / 'sms_spam_no_header.csv' 
MODEL_PATH = BASE_DIR / 'models' / 'lr_model.joblib'
VECTORIZER_PATH = BASE_DIR / 'models' / 'vectorizer.joblib'

TEST_SPAM_MSG = "Congratulations! You have won $1000 cash. Claim your prize now! Text back 'FREE' to 8888"
TEST_HAM_MSG = "Hey, just finished the meeting. Can we review the project notes at 4pm?"

# --- å¿«å–å‡½å¼ ---

@st.cache_resource
def load_and_cache_data_pipeline(test_size, seed):
    """è¼‰å…¥ã€å‰è™•ç†ä¸¦åˆ‡å‰²æ•¸æ“šé›†ï¼Œå¿«å–çµæœã€‚"""
    
    # å°‡ Path ç‰©ä»¶è½‰æ›ç‚ºå­—ä¸²å‚³éçµ¦ pandas
    data_path_str = str(DATA_PATH)
    
    X_train_vec, X_test_vec, y_train, y_test, vectorizer, X_test_raw = \
        load_and_preprocess_data(
            data_path_str, 
            test_size=test_size, 
            random_state=seed
        )
    if X_train_vec is None:
         # æ•¸æ“šè¼‰å…¥å¤±æ•—çš„éŒ¯èª¤è¨Šæ¯å·²ç¶“åœ¨ utils.py ä¸­è™•ç†ï¼Œé€™è£¡åªè¿”å› None
         return None, None, None, None, None, None
         
    return X_train_vec, X_test_vec, y_train, y_test, vectorizer, X_test_raw

@st.cache_resource
def load_model_and_vectorizer():
    """è¼‰å…¥æ¨¡å‹å’Œå‘é‡åŒ–å·¥å…·ã€‚"""
    # ç¢ºä¿ models ç›®éŒ„å­˜åœ¨
    os.makedirs(str(BASE_DIR / 'models'), exist_ok=True) 
    
    model_path_str = str(MODEL_PATH)
    vectorizer_path_str = str(VECTORIZER_PATH)
    
    if not os.path.exists(model_path_str) or not os.path.exists(vectorizer_path_str):
        return None, None
    
    try:
        model = joblib.load(model_path_str)
        vectorizer = joblib.load(vectorizer_path_str)
        return model, vectorizer
    except Exception as e:
        # st.error(f"è¼‰å…¥æ¨¡å‹æˆ–å‘é‡åŒ–å·¥å…·å¤±æ•—: {e}") # é¿å…è¼‰å…¥å¤±æ•—æ™‚ä¸€ç›´å½ˆå‡ºéŒ¯èª¤
        return None, None

# --- Streamlit æ‡‰ç”¨ç¨‹å¼ä¸»é«” ---

st.set_page_config(layout="wide", page_title="Spam/Ham åˆ†é¡å™¨å„€è¡¨æ¿")

st.title("ğŸ“§ Spam/Ham Classifier â€” Phase 4 Visualizations")
st.subheader("äº’å‹•å¼æ•¸æ“šåˆ†ä½ˆã€ç‰¹å¾µå’Œæ¨¡å‹æ€§èƒ½å„€è¡¨æ¿")

# -----------------------------------------------------
# å´é‚Šæ¬„ï¼šè¼¸å…¥æ§åˆ¶é … 
# -----------------------------------------------------
st.sidebar.header("Inputs (è¼¸å…¥æ§åˆ¶é …)")

st.sidebar.selectbox("Dataset CSV (æ•¸æ“šé›†)", [str(DATA_PATH)]) 
st.sidebar.selectbox("Label column (æ¨™ç±¤æ¬„ä½)", ["col_0"])
st.sidebar.selectbox("Text column (æ–‡æœ¬æ¬„ä½)", ["col_1"])

st.sidebar.text_input("Models dir", "models")

test_size = st.sidebar.slider("Test size (æ¸¬è©¦é›†å¤§å°)", 0.05, 0.50, 0.20, 0.05)
seed = st.sidebar.number_input("Seed (éš¨æ©Ÿç¨®å­)", 0, 100, 42)
decision_threshold = st.sidebar.slider("Decision threshold (æ±ºç­–é–¾å€¼)", 0.0, 1.0, 0.50, 0.05)


# -----------------------------------------------------
# ä¸»é¢æ¿ï¼šè¼‰å…¥æ•¸æ“šå’Œæ¨¡å‹
# -----------------------------------------------------

# åˆå§‹è¼‰å…¥æ•¸æ“šå’Œæ¨¡å‹
X_train_vec, X_test_vec, y_train, y_test, vectorizer, X_test_raw = \
    load_and_cache_data_pipeline(test_size, seed)

model, vectorizer_loaded = load_model_and_vectorizer()


# --- é‡æ–°è¨“ç·´æ¨¡å‹æŒ‰éˆ• (ä¿®æ­£å¿«å–è¡çªçš„é‚è¼¯) ---
if st.button("é‡æ–°è¨“ç·´æ¨¡å‹ (Logistic Regression)"):
    
    # æ­¥é©Ÿ 1: å¼·åˆ¶æ¸…é™¤æ‰€æœ‰ç›¸é—œå¿«å–ï¼Œç¢ºä¿è¨“ç·´æ™‚ç²å–æœ€æ–°çš„ Vectorizer
    load_and_cache_data_pipeline.clear() 
    load_model_and_vectorizer.clear()
    
    # é‡æ–°é‹è¡Œæ•¸æ“šç®¡é“ä»¥ç¢ºä¿æˆ‘å€‘æ‹¿åˆ°æœ€æ–°çš„ X_train_vec å’Œ vectorizer
    X_train_vec, X_test_vec, y_train, y_test, vectorizer, X_test_raw = \
        load_and_cache_data_pipeline(test_size, seed)
    
    if X_train_vec is not None and y_train is not None:
        try:
            # æ­¥é©Ÿ 2: è¨“ç·´æ¨¡å‹ä¸¦å„²å­˜
            trained_model = train_and_save_model(X_train_vec, y_train, model_type='LogisticRegression', model_path=str(MODEL_PATH))
            joblib.dump(vectorizer, str(VECTORIZER_PATH)) # å„²å­˜ vectorizer
            
            st.success("æ¨¡å‹å’Œå‘é‡åŒ–å·¥å…·å·²é‡æ–°è¨“ç·´ä¸¦å„²å­˜ï¼")
            st.rerun() # é‡æ–°é‹è¡Œä»¥è¼‰å…¥æ–°æ¨¡å‹
        except Exception as e:
            st.error(f"æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
    else:
        st.error("è¨“ç·´å¤±æ•—ï¼šç„¡æ³•è¼‰å…¥æ•¸æ“šï¼Œè«‹æª¢æŸ¥æ•¸æ“šé›†æª”æ¡ˆã€‚")


# æª¢æŸ¥æ•¸æ“šæ˜¯å¦å¯ç”¨ 
if X_train_vec is None:
    st.error("ç„¡æ³•é¡¯ç¤ºæ•¸æ“šæ¦‚è¦½ï¼Œè«‹æª¢æŸ¥æ•¸æ“šé›†æª”æ¡ˆã€‚")
    st.stop()


# -----------------------------------------------------
# æ•¸æ“šæ¦‚è¦½ (Data Overview) 
# -----------------------------------------------------
st.header("Data Overview (æ•¸æ“šæ¦‚è¦½)")

try:
    original_df = pd.read_csv(str(DATA_PATH), encoding='latin-1', header=None, names=['col_0', 'col_1'])
    class_counts = original_df['col_0'].value_counts()
except Exception:
    st.error("è¼‰å…¥æ•¸æ“šé›†å¤±æ•—ã€‚")
    st.stop() 

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("Class distribution (é¡åˆ¥åˆ†ä½ˆ)")
    fig_dist = go.Figure(data=[go.Bar(x=class_counts.index, y=class_counts.values)])
    fig_dist.update_layout(xaxis_title="é¡åˆ¥", yaxis_title="è¨ˆæ•¸", height=400)
    st.plotly_chart(fig_dist, use_container_width=True) 

with col2:
    st.subheader("Token replacements in cleaned text (è¿‘ä¼¼)")
    token_replacements = pd.DataFrame({
        'Token': ['<URL>', '<EMAIL>', '<PHONE>', '<NUM>'],
        'Count': ['æœªå¯¦ç¾', 'æœªå¯¦ç¾', 'æœªå¯¦ç¾', 'å·²ç§»é™¤'] 
    })
    st.dataframe(token_replacements, hide_index=True)


st.markdown("---")
# -----------------------------------------------------
# æ¨¡å‹æ•ˆèƒ½æŒ‡æ¨™ 
# -----------------------------------------------------

st.header("Model Performance (æ¨¡å‹æ€§èƒ½)")

if model is not None and vectorizer_loaded is not None and X_test_vec is not None:
    try:
        # é‹è¡Œè©•ä¼°
        metrics = evaluate_model(model, X_test_vec, y_test, threshold=decision_threshold)
        cm = metrics['confusion_matrix']

        st.subheader("Model Performance (Test)")
        col3, col4 = st.columns([1, 1])

        with col3:
            st.markdown("##### Confusion Matrix")
            cm_df = pd.DataFrame(cm, 
                                index=['true_0 (ham)', 'true_1 (spam)'], 
                                columns=['pred_0 (ham)', 'pred_1 (spam)'])
            st.dataframe(cm_df)
            
            st.markdown(f"**Precision (ç²¾ç¢ºåº¦):** `{metrics['precision']:.4f}`")
            st.markdown(f"**Recall (å¬å›ç‡):** `{metrics['recall']:.4f}`")
            st.markdown(f"**F1 Score (F1 åˆ†æ•¸):** `{metrics['f1']:.4f}`")


        with col4:
            # --- ROC å’Œ Precision-Recall æ›²ç·š ---
            st.subheader("ROC & Precision-Recall Curves")
            
            fig_curves = make_subplots(rows=1, cols=2, subplot_titles=("ROC", "Precision-Recall"))
            
            # ROC æ›²ç·š
            fig_curves.add_trace(go.Scatter(x=metrics['fpr'], y=metrics['tpr'], mode='lines', name='ROC Curve'), row=1, col=1)
            fig_curves.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(dash='dash', color='gray'), name='Random'), row=1, col=1)
            fig_curves.update_xaxes(title_text="FPR (å‡é™½æ€§ç‡)", row=1, col=1)
            fig_curves.update_yaxes(title_text="TPR (çœŸé™½æ€§ç‡)", row=1, col=1)

            # Precision-Recall æ›²ç·š
            fig_curves.add_trace(go.Scatter(x=metrics['recall_vals'], y=metrics['precision_vals'], mode='lines', name='PR Curve'), row=1, col=2)
            fig_curves.update_xaxes(title_text="Recall (å¬å›ç‡)", row=1, col=2)
            fig_curves.update_yaxes(title_text="Precision (ç²¾ç¢ºåº¦)", row=1, col=2)

            fig_curves.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig_curves, use_container_width=True)

    except ValueError as ve:
        st.error(f"æ¨¡å‹è©•ä¼°å¤±æ•—: {ve}")
        st.warning("å¯èƒ½æ˜¯æ¨¡å‹èˆ‡ç‰¹å¾µæ•¸ä¸åŒ¹é…ã€‚è«‹é»æ“Šä¸Šæ–¹çš„ 'é‡æ–°è¨“ç·´æ¨¡å‹' æŒ‰éˆ•ã€‚")
    except Exception as e:
        st.error(f"æ¨¡å‹è©•ä¼°æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")

else:
    st.info("æ¨¡å‹æ€§èƒ½å€å¡Šï¼šè«‹å…ˆæˆåŠŸè¨“ç·´å’Œè¼‰å…¥æ¨¡å‹å¾Œï¼Œæ­¤è™•æ‰æœƒé¡¯ç¤ºæ€§èƒ½æŒ‡æ¨™å’Œåœ–è¡¨ã€‚")

st.markdown("---")
# -----------------------------------------------------
# é–¾å€¼æƒæè¡¨æ ¼ (Threshold Sweep Table) 
# -----------------------------------------------------

st.header("Threshold sweep (precision/recall/f1)")

if model is not None and X_test_vec is not None and y_test is not None:
    
    try:
        # å‰µå»ºä¸€å€‹é–¾å€¼ç¯„åœ
        thresholds_to_test = np.arange(0.1, 0.91, 0.05) 

        sweep_results = []
        
        # ç²å–æ¸¬è©¦é›†ä¸Šçš„é æ¸¬æ©Ÿç‡
        y_proba = model.predict_proba(X_test_vec)[:, 1]
        
        for t in thresholds_to_test:
            t = round(t, 2)
            
            y_pred = (y_proba >= t).astype(int)
            
            p = precision_score(y_test, y_pred, zero_division=0)
            r = recall_score(y_test, y_pred, zero_division=0)
            f = f1_score(y_test, y_pred, zero_division=0)
            
            sweep_results.append({
                'threshold': t,
                'precision': round(p, 4),
                'recall': round(r, 4),
                'f1': round(f, 4)
            })

        sweep_df = pd.DataFrame(sweep_results)
        
        # é¡¯ç¤ºè¡¨æ ¼ (æ¨¡ä»¿åœ– e7d5c3)
        st.dataframe(sweep_df, hide_index=True)
        
    except Exception as e:
        st.error(f"é–¾å€¼æƒæå¤±æ•—: {e}")
        st.warning("è«‹ç¢ºä¿æ¨¡å‹å’Œæ•¸æ“šå·²æ­£ç¢ºè¼‰å…¥ã€‚")
        
else:
    st.info("é–¾å€¼æƒæè¡¨æ ¼ï¼šè«‹å…ˆæˆåŠŸè¨“ç·´å’Œè¼‰å…¥æ¨¡å‹ã€‚")

st.markdown("---")
# -----------------------------------------------------
# è©å½™åˆ†æ (Top Tokens by Class) 
# -----------------------------------------------------

st.header("Token Analysis (è©å½™åˆ†æ)")

if vectorizer_loaded is not None and X_train_vec is not None and y_train is not None:
    
    top_n = st.slider("Top N tokens (ç†±é–€ N è©å½™)", 5, 50, 20)
    
    try:
        top_tokens_ham, top_tokens_spam = get_top_tokens(vectorizer_loaded, X_train_vec, y_train, top_n)

        fig_tokens = make_subplots(rows=1, cols=2, 
                                subplot_titles=("Class: ham (éåƒåœ¾éƒµä»¶)", "Class: spam (åƒåœ¾éƒµä»¶)"))

        # Ham è©å½™åœ–
        fig_tokens.add_trace(go.Bar(
            x=top_tokens_ham['frequency'], 
            y=top_tokens_ham['token'], 
            orientation='h', 
            name='Ham',
            marker_color='#1f77b4'
        ), row=1, col=1)

        # Spam è©å½™åœ–
        fig_tokens.add_trace(go.Bar(
            x=top_tokens_spam['frequency'], 
            y=top_tokens_spam['token'], 
            orientation='h', 
            name='Spam',
            marker_color='#d62728'
        ), row=1, col=2)

        fig_tokens.update_layout(height=600, showlegend=False, title_text=f"Top {top_n} Tokens by Class (TF-IDF Sum)")
        fig_tokens.update_yaxes(autorange="reversed", row=1, col=1)
        fig_tokens.update_yaxes(autorange="reversed", row=1, col=2)
        fig_tokens.update_xaxes(title_text="frequency (TF-IDF Sum)", row=1, col=1)
        fig_tokens.update_xaxes(title_text="frequency (TF-IDF Sum)", row=1, col=2)
        
        st.plotly_chart(fig_tokens, use_container_width=True)
        
    except Exception as e:
        st.error(f"è©å½™åˆ†æåœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")
        st.warning("è«‹ç¢ºä¿æ¨¡å‹å’Œæ•¸æ“šå·²æ­£ç¢ºè¼‰å…¥ã€‚")

else:
    st.info("è©å½™åˆ†æå€å¡Šï¼šè«‹å…ˆæˆåŠŸè¨“ç·´å’Œè¼‰å…¥æ¨¡å‹å¾Œï¼Œæ­¤è™•æ‰æœƒé¡¯ç¤ºè©å½™åˆ†æåœ–è¡¨ã€‚")

st.markdown("---")
# -----------------------------------------------------
# å¯¦æ™‚æ¨è«– (Live Inference) 
# -----------------------------------------------------

st.header("Live Inference (å¯¦æ™‚æ¨è«–)")

if model is not None and vectorizer_loaded is not None:
    
    # æŒ‰éˆ•å€
    col_btn1, col_btn2 = st.columns([1, 1])
    with col_btn1:
        if st.button("Use spam example"):
            st.session_state['message'] = TEST_SPAM_MSG
    with col_btn2:
        if st.button("Use ham example"):
            st.session_state['message'] = TEST_HAM_MSG

    # è¼¸å…¥æ¡† (ä½¿ç”¨ session_state ä¿æŒæŒ‰éˆ•å’Œè¼¸å…¥æ¡†åŒæ­¥)
    if 'message' not in st.session_state:
        st.session_state['message'] = ""

    input_message = st.text_area(
        "Enter a message to classify (è¼¸å…¥è¦åˆ†é¡çš„è¨Šæ¯)", 
        st.session_state['message'],
        height=150
    )

    # é æ¸¬æŒ‰éˆ•
    if st.button("Predict (é æ¸¬)", key="predict_btn") and input_message:
        
        # åŸ·è¡Œé æ¸¬
        prediction, proba = predict_message(
            input_message, 
            vectorizer_loaded, 
            model, 
            threshold=decision_threshold
        )
        
        st.subheader("Prediction Result (é æ¸¬çµæœ)")
        
        # é¡¯ç¤ºçµæœ
        if prediction == 'spam':
            st.error(f"åˆ†é¡çµæœï¼š**{prediction.upper()}** (åƒåœ¾éƒµä»¶)")
        else:
            st.success(f"åˆ†é¡çµæœï¼š**{prediction.upper()}** (éåƒåœ¾éƒµä»¶)")
            
        st.markdown(f"è©²è¨Šæ¯æ˜¯ **SPAM** çš„æ©Ÿç‡ç‚º: **`{proba:.4f}`**")
        st.markdown(f"ä½¿ç”¨çš„æ±ºç­–é–¾å€¼ (Decision Threshold): **`{decision_threshold:.2f}`**")
else:
    st.info("å¯¦æ™‚æ¨è«–å€å¡Šï¼šè«‹å…ˆæˆåŠŸè¨“ç·´å’Œè¼‰å…¥æ¨¡å‹å¾Œï¼Œæ­¤è™•æ‰æœƒå•Ÿç”¨ã€‚")